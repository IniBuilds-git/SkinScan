{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision matplotlib numpy\n",
    "%pip install torchvision  \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ced33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set paths\n",
    "DATASET_PATH = '../dataset/raw'\n",
    "TRAIN_PATH = os.path.join(DATASET_PATH, 'train')\n",
    "TEST_PATH = os.path.join(DATASET_PATH, 'test')\n",
    "MODEL_SAVE_PATH = '../data/models'\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set hyperparameters\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 25 \n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-4  # Add regularization\n",
    "FEATURE_EXTRACT = False  # Set to False to fine-tune all layers\n",
    "\n",
    "# 1. This calculates the dataset statistics for better normalization\n",
    "def calculate_dataset_stats(data_loader):\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    total_images = 0\n",
    "    \n",
    "    for images, _ in tqdm(data_loader, desc=\"Calculating dataset statistics\"):\n",
    "        batch_size = images.size(0)\n",
    "        images = images.view(batch_size, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images += batch_size\n",
    "    \n",
    "    mean /= total_images\n",
    "    std /= total_images\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# First use a simple transform to calculate statistics\n",
    "simple_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "initial_dataset = datasets.ImageFolder(root=TRAIN_PATH, transform=simple_transforms)\n",
    "initial_loader = DataLoader(initial_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "try:\n",
    "    print(\"Calculating dataset statistics...\")\n",
    "    mean, std = calculate_dataset_stats(initial_loader)\n",
    "    print(f\"Dataset mean: {mean}, std: {std}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating statistics: {e}. Using default values.\")\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])  #Using ImageNet stats as fallback\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "# 2. This creates improved data transforms with proper normalization and more augmentations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(), \n",
    "    transforms.RandomRotation(20),  \n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# 3. This creates the dataset and address class imbalance\n",
    "train_dataset = datasets.ImageFolder(root=TRAIN_PATH, transform=train_transforms)\n",
    "test_dataset = datasets.ImageFolder(root=TEST_PATH, transform=test_transforms)\n",
    "\n",
    "# Calculating class weights for handling imbalance\n",
    "class_counts = [0] * len(train_dataset.classes)\n",
    "for _, index in train_dataset.samples:\n",
    "    class_counts[index] += 1\n",
    "\n",
    "# Creating weights for each sample in the dataset\n",
    "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
    "sample_weights = [class_weights[class_idx] for _, class_idx in train_dataset.samples]\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Creating data loaders with sampler for training\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# 4. This is a function to set model parameters to require or not require gradients\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# 5. This initaites the model (ResNet50 for better performance)\n",
    "def initialize_model(num_classes, feature_extract=False):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    set_parameter_requires_grad(model, feature_extract)\n",
    "    \n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.3), \n",
    "        nn.Linear(num_ftrs, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "model = initialize_model(len(class_names), feature_extract=FEATURE_EXTRACT)\n",
    "model = model.to(device)\n",
    "\n",
    "# 6. This defines loss function with class weights\n",
    "class_weights_tensor = class_weights.to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# 7. This defines optimizers and schedulers\n",
    "if FEATURE_EXTRACT:\n",
    "    # Only optimize the classifier parameters\n",
    "    params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
    "else:\n",
    "    # Different learning rates for layers\n",
    "    # Lower learning rate for pre-trained layers, higher for the new classifier\n",
    "    params_to_update = [\n",
    "        {'params': model.fc.parameters(), 'lr': LEARNING_RATE},\n",
    "        {'params': [p for n, p in model.named_parameters() if 'fc' not in n], 'lr': LEARNING_RATE/10}\n",
    "    ]\n",
    "\n",
    "optimizer = optim.AdamW(params_to_update, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler - reduce LR when validation loss plateaus\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# 8. This function is to train the model with improvements\n",
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Track best model weights\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # Track history for plotting\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloader = dataloaders['train']\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                dataloader = dataloaders['val']\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Progress bar for iterations\n",
    "            loop = tqdm(dataloader, desc=f\"{phase.capitalize()} Progress\")\n",
    "            \n",
    "            # Iterate over data\n",
    "            for inputs, labels in loop:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        # Gradient clipping to prevent exploding gradients\n",
    "                        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                # Update progress bar\n",
    "                loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # Save history for plotting\n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item())\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "                # Update the learning rate scheduler\n",
    "                scheduler.step(epoch_loss)\n",
    "            \n",
    "            # Deep copy the model if best validation accuracy\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                # Save the best model so far\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_SAVE_PATH, 'best_model.pth'))\n",
    "                print(f\"Saved new best model with accuracy: {best_acc:.4f}\")\n",
    "        \n",
    "        # Save checkpoint every 5 epochs for long training\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "            }, os.path.join(MODEL_SAVE_PATH, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "    \n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f'\\nTraining completed in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best validation accuracy: {best_acc:.4f}')\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss over epochs')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy over epochs')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(MODEL_SAVE_PATH, 'training_history.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "# 9. Train the model\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': test_loader\n",
    "}\n",
    "\n",
    "model, history = train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# 10. This is to save the final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'class_names': class_names,\n",
    "    'mean': mean,\n",
    "    'std': std,\n",
    "    'history': history\n",
    "}, os.path.join(MODEL_SAVE_PATH, 'final_model.pth'))\n",
    "\n",
    "print(\"Final model saved to:\", os.path.join(MODEL_SAVE_PATH, 'final_model.pth'))\n",
    "\n",
    "# 11. Perform validation and create confusion matrix\n",
    "def validate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(15, 13))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(MODEL_SAVE_PATH, 'final_confusion_matrix.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "print(\"\\nValidating model on test set...\")\n",
    "predictions, true_labels = validate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
