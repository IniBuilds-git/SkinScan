{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../data/models/final_model.pth' \n",
    "RESULTS_PATH = '../results'\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "#  Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for test data\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1cb77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_dir = os.path.join(DATASET_PATH, 'test')\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60fc7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names\n",
    "class_names = test_dataset.classes\n",
    "print(f\"Found {len(class_names)} classes: {class_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc83cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model with the  number of classes\n",
    "model = resnet50(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "# Load saved model parameters\n",
    "try:\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    print(f\"Model loaded from {MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Continuing with an untrained model for testing purposes.\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f560fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_scores = []\n",
    "    \n",
    "    # Track correct predictions and total samples for each class\n",
    "    class_correct = {i: 0 for i in range(len(class_names))}\n",
    "    class_total = {i: 0 for i in range(len(class_names))} \n",
    "    \n",
    "    # Store incorrect examples for analysis\n",
    "    incorrect_examples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Record prediction scores (probabilities)\n",
    "            scores = F.softmax(outputs, dim=1)\n",
    "            \n",
    "            # Extend lists for later analysis\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_scores.extend(scores.cpu().numpy())\n",
    "            \n",
    "            # Update class-wise accuracy tracking\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                prediction = preds[i].item()\n",
    "                class_total[label] += 1\n",
    "                if label == prediction:\n",
    "                    class_correct[label] += 1\n",
    "                else:\n",
    "                    # Store incorrect examples (input tensor, true label, predicted label)\n",
    "                    incorrect_examples.append((\n",
    "                        inputs[i].cpu(),\n",
    "                        label,\n",
    "                        prediction,\n",
    "                        scores[i].cpu().numpy()\n",
    "                    ))\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    accuracy = sum(class_correct.values()) / sum(class_total.values())\n",
    "    \n",
    "    # Calculate class-wise accuracy\n",
    "    class_accuracy = {class_names[i]: class_correct[i]/class_total[i] \n",
    "                      if class_total[i] > 0 else 0 \n",
    "                      for i in range(len(class_names))}\n",
    "    \n",
    "    return {\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'y_scores': y_scores,\n",
    "        'accuracy': accuracy,\n",
    "        'class_accuracy': class_accuracy,\n",
    "        'incorrect_examples': incorrect_examples\n",
    "    }\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "results = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Print overall accuracy\n",
    "print(f\"\\nOverall Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(results['y_true'], results['y_pred'], target_names=class_names, output_dict=True)\n",
    "print(classification_report(results['y_true'], results['y_pred'], target_names=class_names))\n",
    "\n",
    "# Convert report to dataframe for easier handling\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df.to_csv(os.path.join(RESULTS_PATH, 'classification_report.csv'))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(results['y_true'], results['y_pred'])\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_PATH, 'confusion_matrix.png'))\n",
    "plt.show()\n",
    "\n",
    "# Normalize confusion matrix by row (true labels)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Normalized Confusion Matrix (by row)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_PATH, 'confusion_matrix_normalized.png'))\n",
    "plt.show()\n",
    "\n",
    "# Plot class-wise accuracy\n",
    "class_acc = results['class_accuracy']\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(class_acc.keys(), class_acc.values())\n",
    "plt.axhline(y=results['accuracy'], color='r', linestyle='-', label=f'Overall Accuracy: {results[\"accuracy\"]:.2f}')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Class-wise Accuracy')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(RESULTS_PATH, 'class_wise_accuracy.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a955291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to visualize incorrect predictions\n",
    "def plot_incorrect_predictions(incorrect_examples, class_names, num_to_show=10):\n",
    "    \"\"\"\n",
    "    Visualize incorrect predictions\n",
    "    \"\"\"\n",
    "    if not incorrect_examples:\n",
    "        print(\"No incorrect predictions to show.\")\n",
    "        return\n",
    "    \n",
    "    # Sort by confidence of wrong prediction (most confident mistakes first)\n",
    "    sorted_examples = sorted(incorrect_examples, \n",
    "                            key=lambda x: x[3][x[2]], \n",
    "                            reverse=True)\n",
    "    \n",
    "    # Take the top num_to_show examples\n",
    "    examples_to_plot = sorted_examples[:num_to_show]\n",
    "    \n",
    "    # Plot the examples\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Inverse normalization function to display images properly\n",
    "    inv_normalize = transforms.Compose([\n",
    "        transforms.Normalize(\n",
    "            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "            std=[1/0.229, 1/0.224, 1/0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    for i, (img_tensor, true_label, pred_label, scores) in enumerate(examples_to_plot):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        # Convert tensor to image\n",
    "        img_tensor = inv_normalize(img_tensor)\n",
    "        img = img_tensor.permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Display image\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\\nConf: {scores[pred_label]:.2f}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, 'incorrect_predictions.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Visualize incorrect predictions\n",
    "print(\"\\nVisualizing top confident incorrect predictions:\")\n",
    "plot_incorrect_predictions(results['incorrect_examples'], class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function visualizes confusion between specific classes\n",
    "def plot_class_confusion(y_true, y_pred, class_names, top_n=5):\n",
    "    \"\"\"\n",
    "    Find the most commonly confused class pairs and visualize them\n",
    "    \"\"\"\n",
    "    # Initialize confusion count dictionary\n",
    "    confusion_pairs = {}\n",
    "    \n",
    "    # Count confusion instances\n",
    "    for true_idx, pred_idx in zip(y_true, y_pred):\n",
    "        if true_idx != pred_idx:\n",
    "            pair = (class_names[true_idx], class_names[pred_idx])\n",
    "            confusion_pairs[pair] = confusion_pairs.get(pair, 0) + 1\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_pairs = sorted(confusion_pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top N pairs\n",
    "    top_pairs = sorted_pairs[:top_n]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    pairs = [f\"{true} → {pred}\" for (true, pred), _ in top_pairs]\n",
    "    counts = [count for _, count in top_pairs]\n",
    "    \n",
    "    plt.bar(pairs, counts)\n",
    "    plt.xlabel('Confusion Pair (True → Predicted)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Top {top_n} Most Confused Class Pairs')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, 'top_confused_classes.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return top_pairs\n",
    "\n",
    "# Plot top confused class pairs\n",
    "print(\"\\nAnalyzing most commonly confused classes:\")\n",
    "confused_pairs = plot_class_confusion(results['y_true'], results['y_pred'], class_names)\n",
    "for (true_class, pred_class), count in confused_pairs:\n",
    "    print(f\"True: {true_class}, Predicted: {pred_class}, Count: {count}\")\n",
    "\n",
    "# Calculate and plot precision, recall, and F1 score for each class\n",
    "precision = {}\n",
    "recall = {}\n",
    "f1_score = {}\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    precision[class_name] = report['macro avg']['precision'] \n",
    "    recall[class_name] = report['macro avg']['recall']\n",
    "    f1_score[class_name] = report['macro avg']['f1-score']\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1_score\n",
    "})\n",
    "\n",
    "# Plot precision, recall, and F1 score\n",
    "plt.figure(figsize=(12, 8))\n",
    "metrics_df.plot(kind='bar', figsize=(14, 8))\n",
    "plt.title('Precision, Recall, and F1 Score by Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_PATH, 'precision_recall_f1.png'))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel evaluation complete. Results saved to:\", RESULTS_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
